import requests
from bs4 import BeautifulSoup
import json
import os
from pathlib import Path
import time

# è¨­å®šè¦æŠ“å–çš„æ³•è¦ PCODE
TARGET_LAWS = {
    "C0000001": "ä¸­è¯æ°‘åœ‹åˆ‘æ³•",
    "K0040012": "é“è·¯äº¤é€šç®¡ç†è™•ç½°æ¢ä¾‹",
    "B0000001": "æ°‘æ³•"
}

# æ”¹ç”¨ç¶²é ç‰ˆé€£çµ (é€™æ˜¯äººé¡çœ‹çš„é é¢ï¼Œéå¸¸ç©©å®š)
BASE_URL = "https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode={}"

# è¨­å®šè¼¸å‡ºè·¯å¾‘ (è·Ÿ ingest.py å°æ¥)
current_dir = Path(__file__).parent
DATA_DIR = current_dir / "data"
OUTPUT_FILE = DATA_DIR / "laws.json"

def fetch_and_save_laws():
    if not DATA_DIR.exists():
        os.makedirs(DATA_DIR)
        print(f"ğŸ“ å»ºç«‹è³‡æ–™å¤¾: {DATA_DIR}")

    all_law_articles = []

    print("ğŸš€ é–‹å§‹å¾å…¨åœ‹æ³•è¦è³‡æ–™åº«æŠ“å–è³‡æ–™...")

    for pcode, law_name in TARGET_LAWS.items():
        url = BASE_URL.format(pcode)
        print(f"ğŸ“¡ æ­£åœ¨è®€å–: {law_name}...")
        
        try:
            # å½è£æˆç€è¦½å™¨ (é¿å…è¢«æª”)
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            
            # ä½¿ç”¨ BeautifulSoup è§£æç¶²é 
            soup = BeautifulSoup(response.text, "html.parser")
            
            # æ‰¾åˆ°æ‰€æœ‰çš„æ¢æ–‡å€å¡Š (åœ¨ç¶²é ä¸­é€šå¸¸æ˜¯ tr æˆ– div)
            # å…¨åœ‹æ³•è¦è³‡æ–™åº«çš„çµæ§‹ï¼šæ¢è™Ÿåœ¨ .col-noï¼Œå…§å®¹åœ¨ .col-data
            articles = soup.find_all("div", class_="row")
            
            count = 0
            for row in articles:
                # æ‰¾æ¢è™Ÿ
                col_no = row.find("div", class_="col-no")
                # æ‰¾å…§å®¹
                col_data = row.find("div", class_="col-data")
                
                if col_no and col_data:
                    article_no = col_no.get_text(strip=True)
                    content = col_data.get_text(strip=True)
                    
                    # éæ¿¾æ‰å»¢æ­¢æˆ–åˆªé™¤çš„æ¢æ–‡
                    if "åˆªé™¤" in content or "å»¢æ­¢" in content:
                        continue
                        
                    # æ•´ç†è³‡æ–™
                    law_data = {
                        "id": f"{pcode}_{article_no}", 
                        "text": f"{law_name} {article_no}ï¼š{content}",
                        "category": law_name
                    }
                    all_law_articles.append(law_data)
                    count += 1
            
            print(f"   âœ… æˆåŠŸæŠ“å– {count} æ¢æ¢æ–‡")
            
            # ç¦®è²Œæ€§æš«åœä¸€ä¸‹ï¼Œä¸è¦è«‹æ±‚å¤ªå¿«
            time.sleep(1)

        except Exception as e:
            print(f"   âŒ å¤±æ•—: {e}")

    # å­˜æª”
    print(f"ğŸ’¾ æ­£åœ¨å„²å­˜ {len(all_law_articles)} æ¢è³‡æ–™åˆ° {OUTPUT_FILE}...")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_law_articles, f, ensure_ascii=False, indent=2)

    print("ğŸ‰ å®Œæˆï¼laws.json å·²ç”Ÿæˆï¼Œç¾åœ¨è«‹åŸ·è¡Œ ingest.py ä¾†å»ºç«‹å¤§è…¦ï¼")

if __name__ == "__main__":
    fetch_and_save_laws()
