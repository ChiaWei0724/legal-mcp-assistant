import os
import json
import jieba
from rank_bm25 import BM25Okapi
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai
from dotenv import load_dotenv
from pathlib import Path

# --- 1. ç’°å¢ƒè¨­å®š ---
base_path = Path(__file__).parent.parent
load_dotenv(base_path / ".env")

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError("âŒ æ‰¾ä¸åˆ° GOOGLE_API_KEY")

genai.configure(api_key=GOOGLE_API_KEY)

# --- 2. åˆå§‹åŒ– ChromaDB ---
current_dir = Path(__file__).parent
DB_PATH = current_dir / "chroma_db"
client = chromadb.PersistentClient(path=str(DB_PATH))

google_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(
    api_key=GOOGLE_API_KEY,
    model_name="models/text-embedding-004",
    task_type="retrieval_query"
)

try:
    collection = client.get_collection(
        name="legal_knowledge",
        embedding_function=google_ef
    )
    print(f"âœ… å‘é‡è³‡æ–™åº«é€£ç·šæˆåŠŸï¼ŒåŒ…å« {collection.count()} æ¢æ³•è¦")
except Exception as e:
    print(f"âŒ è³‡æ–™åº«é€£ç·šå¤±æ•—: {e}")
    collection = client.get_or_create_collection(name="legal_knowledge", embedding_function=google_ef)

# --- 3. åˆå§‹åŒ– BM25 ---
print("â³ æ­£åœ¨è¼‰å…¥ BM25 ç´¢å¼•...")
DATA_PATH = current_dir / "data" / "laws.json"
all_laws = []
bm25 = None

if DATA_PATH.exists():
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        all_laws = json.load(f)
    tokenized_corpus = [list(jieba.cut(doc['text'])) for doc in all_laws]
    bm25 = BM25Okapi(tokenized_corpus)
    print(f"âœ… BM25 ç´¢å¼•å»ºç«‹å®Œæˆï¼")
else:
    print("âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° laws.json")

# --- 4. FastAPI ---
app = FastAPI(title="Legal MCP Assistant API")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str

# --- æ ¸å¿ƒï¼šåŒç¾©è©æ“´å±•å­—å…¸ (Synonym Expansion) ---
# é€™æ˜¯ç‚ºäº†å½Œè£œã€Œä¿—ç¨±ã€èˆ‡ã€Œæ³•æ¢åŸæ–‡ã€çš„å·®è·
def expand_synonyms(query: str) -> str:
    synonyms = {
        "é…’æ¸¬": "é…’ç²¾æ¿ƒåº¦ æ¸¬è©¦ æª¢å®š æ‹’çµ•",
        "ä¹ç­–": "é…’ç²¾æ¿ƒåº¦ æ¸¬è©¦ æª¢å®š æ‹’çµ•", # é‡å°ç‰¹å®šéŒ¯å­—ç›´æ¥è™•ç†
        "é—–ç´…ç‡ˆ": "è™ŸèªŒ ç®¡åˆ¶ é—–è¶Š äº¤å²”è·¯å£",
        "ç´…ç‡ˆ": "è™ŸèªŒ ç®¡åˆ¶",
        "è¶…é€Ÿ": "è¡Œè»Šé€Ÿåº¦ è¶…é æœ€é«˜æ™‚é€Ÿ",
        "ç„¡ç…§": "æœªé ˜æœ‰ é§•é§›åŸ·ç…§",
        "å·æ‹¿": "ç«Šç›œ ç«Šå–",
        "æ‰“äºº": "å‚·å®³ç½ª èº«é«” å¥åº·",
        "ç½µäºº": "å…¬ç„¶ä¾®è¾± èª¹è¬—",
        "æœªç¦®è®“": "æš«åœ è®“ è¡Œäºº å…ˆè¡Œ",
        "å®‰å…¨å¸½": "æœªä¾è¦å®š æˆ´å®‰å…¨å¸½",
    }
    
    expanded = query
    for key, value in synonyms.items():
        if key in query:
            expanded += f" {value}"
    
    return expanded

# --- æ ¸å¿ƒï¼šé›™è»Œæœå°‹ ---
def hybrid_search(query: str):
    # 1. å…ˆåšåŒç¾©è©æ“´å±•
    expanded_query = expand_synonyms(query)
    print(f"ğŸ” æ“´å±•å¾Œæœå°‹è©: {expanded_query}")
    
    final_docs = []
    seen_ids = set()
    
    # è»Œé“ A: BM25 (æŠ“é—œéµå­—) - æé«˜åˆ° 30 ç­†
    if bm25:
        tokenized_query = list(jieba.cut(expanded_query))
        bm25_results = bm25.get_top_n(tokenized_query, all_laws, n=30)
        for doc in bm25_results:
            if doc['id'] not in seen_ids:
                final_docs.append({"text": doc['text'], "id": doc['id'], "score": 0.8})
                seen_ids.add(doc['id'])

    # è»Œé“ B: Vector (æŠ“èªæ„) - æé«˜åˆ° 30 ç­†
    vector_results = collection.query(
        query_texts=[expanded_query],
        n_results=30
    )
    
    if vector_results['documents'] and vector_results['documents'][0]:
        for i, doc_text in enumerate(vector_results['documents'][0]):
            doc_id = vector_results['ids'][0][i]
            if doc_id not in seen_ids:
                final_docs.append({"text": doc_text, "id": doc_id, "score": 0.7})
                seen_ids.add(doc_id)
            else:
                # é‡ç–ŠåŠ åˆ†
                for item in final_docs:
                    if item['id'] == doc_id:
                        item['score'] += 0.5

    # è»Œé“ C: é—œéµå­—æš´åŠ›åŠ æ¬Š
    # ç¢ºä¿åŒ…å«ã€Œä½¿ç”¨è€…åŸå§‹é—œéµå­—ã€çš„æ³•æ¢æ’åœ¨æœ€å‰é¢
    keywords = list(jieba.cut(query))
    for item in final_docs:
        for kw in keywords:
            if len(kw) > 1 and kw in item['text']:
                item['score'] += 0.3

    final_docs.sort(key=lambda x: x['score'], reverse=True)
    
    # å–å‰ 15 æ¢çµ¦ AI (Gemini Context Window å¤ å¤§ï¼Œå¤šçµ¦ä¸€é»æ²’é—œä¿‚)
    print(f"ğŸ† æœ€çµ‚å‰ 3 å: {[item['id'] for item in final_docs[:3]]}")
    return "\n\n".join([item['text'] for item in final_docs[:15]])

def query_gemini_rag(user_question: str):
    print(f"ğŸ‘¤ ä½¿ç”¨è€…: {user_question}")

    # Step 1: AI æ”¹å¯« (ä¿®éŒ¯å­—ã€è£œä¸»è©)
    rewrite_model = genai.GenerativeModel('gemini-2.0-flash')
    rewrite_prompt = f"""
    è«‹å°‡ä½¿ç”¨è€…å•é¡Œæ”¹å¯«ç‚ºç²¾æº–çš„æ³•å¾‹æœå°‹å­—ä¸²ã€‚
    1. ä¿®æ­£éŒ¯å­— (å¦‚: æ‹’çµ•ä¹ç­– -> æ‹’çµ•é…’æ¸¬)ã€‚
    2. è£œå……ä¸»è© (å¦‚: æœªç¦®è®“ -> æ±½è»Šæ©Ÿè»Šé§•é§›æœªç¦®è®“è¡Œäºº)ã€‚
    3. åªè¼¸å‡ºæ”¹å¯«å¾Œçš„å­—ä¸²ã€‚
    å•é¡Œ: {user_question}
    """
    rewritten_query = rewrite_model.generate_content(rewrite_prompt).text.strip()
    print(f"âœ¨ AI æ”¹å¯«: {rewritten_query}")

    # Step 2: æœå°‹
    context_text = hybrid_search(rewritten_query)
    
    if not context_text:
        context_text = "ç„¡ç›¸é—œæ³•æ¢ã€‚"

    # Step 3: ç”Ÿæˆå›ç­” (åŠ å…¥ä¸­æ–‡æ•¸å­—è½‰æ›æŒ‡ä»¤)
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å°ç£æ³•å¾‹ AI é¡§å•ã€‚
    
    ã€ç›¸é—œæ³•è¦ã€‘ï¼š
    {context_text}
    
    ã€ä½¿ç”¨è€…å•é¡Œã€‘ï¼š
    {user_question} (æ„åœ–: {rewritten_query})
    
    ã€å›ç­”è¦å‰‡ (Markdown)ã€‘ï¼š
    1. **ä¸­æ–‡æ•¸å­—è½‰æ›**ï¼šæ³•è¦ä¸­çš„é‡‘é¡å¸¸å¯«æˆã€Œä¸€åƒå…«ç™¾å…ƒã€ï¼Œè«‹ä½ è½‰æ›ç‚ºã€Œ1800å…ƒã€é¡¯ç¤ºï¼Œæ–¹ä¾¿é–±è®€ã€‚
    2. **ç›´æ¥å›ç­”**ï¼šç¬¬ä¸€è¡Œç›´æ¥çµ¦å‡ºé‡é»ï¼ˆé‡‘é¡ã€åˆ‘æœŸï¼‰ã€‚
    3. **ä¸»å‹•åˆ—èˆ‰**ï¼šè‹¥æœªæŒ‡å®šè»Šç¨®ï¼Œè«‹åˆ—å‡ºã€Œæ©Ÿè»Šã€èˆ‡ã€Œæ±½è»Šã€çš„å€‹åˆ¥ç½°å‰‡ã€‚
    4. **å¼•ç”¨æ³•æ¢**ï¼šæ³•è¦åç¨±èˆ‡æ¢è™Ÿéœ€ç”¨ **ç²—é«”**ã€‚
    5. **æ¢åˆ—å¼åˆ†æ**ï¼šä½¿ç”¨ Bullet pointsã€‚
    6. **å…è²¬è²æ˜**ï¼šæœ€å¾ŒåŠ ä¸Šå¼•ç”¨å€å¡Šã€‚
    """

    answer_model = genai.GenerativeModel('gemini-2.0-flash') 
    response = answer_model.generate_content(system_prompt)
    return response.text

@app.get("/")
def read_root():
    return {"message": "Legal AI Backend is Running!"}

@app.post("/chat")
async def chat(request: ChatRequest):
    try:
        ai_reply = query_gemini_rag(request.message)
        return {"reply": ai_reply}
    except Exception as e:
        print(f"Error: {e}")
        return {"reply": f"ç™¼ç”ŸéŒ¯èª¤: {str(e)}"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)