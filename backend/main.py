import os
import json
import jieba
from rank_bm25 import BM25Okapi
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai
from dotenv import load_dotenv
from pathlib import Path

# --- 1. ç’°å¢ƒè¨­å®š ---
base_path = Path(__file__).parent.parent
load_dotenv(base_path / ".env")

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError("âŒ æ‰¾ä¸åˆ° GOOGLE_API_KEY")

genai.configure(api_key=GOOGLE_API_KEY)

# --- 2. åˆå§‹åŒ– ChromaDB ---
current_dir = Path(__file__).parent
DB_PATH = current_dir / "chroma_db"
client = chromadb.PersistentClient(path=str(DB_PATH))

google_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(
    api_key=GOOGLE_API_KEY,
    model_name="models/text-embedding-004",
    task_type="retrieval_query"
)

try:
    collection = client.get_collection(
        name="legal_knowledge",
        embedding_function=google_ef
    )
    print(f"âœ… å‘é‡è³‡æ–™åº«é€£ç·šæˆåŠŸï¼ŒåŒ…å« {collection.count()} æ¢æ³•è¦")
except Exception as e:
    print(f"âŒ è³‡æ–™åº«é€£ç·šå¤±æ•—: {e}")
    collection = client.get_or_create_collection(name="legal_knowledge", embedding_function=google_ef)

# --- 3. åˆå§‹åŒ– BM25 ---
print("â³ æ­£åœ¨è¼‰å…¥ BM25 ç´¢å¼•...")
DATA_PATH = current_dir / "data" / "laws.json"
all_laws = []
bm25 = None

if DATA_PATH.exists():
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        all_laws = json.load(f)
    tokenized_corpus = [list(jieba.cut(doc['text'])) for doc in all_laws]
    bm25 = BM25Okapi(tokenized_corpus)
    print(f"âœ… BM25 ç´¢å¼•å»ºç«‹å®Œæˆï¼")
else:
    print("âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° laws.json")

# --- 4. FastAPI ---
app = FastAPI(title="Legal MCP Assistant API")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str

# --- æ ¸å¿ƒï¼šåŒç¾©è©æ“´å±•å­—å…¸ (Synonym Expansion) ---
# é€™æ˜¯ç‚ºäº†å½Œè£œã€Œä¿—ç¨±ã€èˆ‡ã€Œæ³•æ¢åŸæ–‡ã€çš„å·®è·
def expand_synonyms(query: str) -> str:
    synonyms = {
        "é…’æ¸¬": "é…’ç²¾æ¿ƒåº¦ æ¸¬è©¦ æª¢å®š æ‹’çµ•",
        "ä¹ç­–": "é…’ç²¾æ¿ƒåº¦ æ¸¬è©¦ æª¢å®š æ‹’çµ•", # é‡å°ç‰¹å®šéŒ¯å­—ç›´æ¥è™•ç†
        "é—–ç´…ç‡ˆ": "è™ŸèªŒ ç®¡åˆ¶ é—–è¶Š äº¤å²”è·¯å£",
        "ç´…ç‡ˆ": "è™ŸèªŒ ç®¡åˆ¶",
        "è¶…é€Ÿ": "è¡Œè»Šé€Ÿåº¦ è¶…é æœ€é«˜æ™‚é€Ÿ",
        "ç„¡ç…§": "æœªé ˜æœ‰ é§•é§›åŸ·ç…§",
        "å·æ‹¿": "ç«Šç›œ ç«Šå–",
        "æ‰“äºº": "å‚·å®³ç½ª èº«é«” å¥åº·",
        "ç½µäºº": "å…¬ç„¶ä¾®è¾± èª¹è¬—",
        "æœªç¦®è®“": "æš«åœ è®“ è¡Œäºº å…ˆè¡Œ",
        "å®‰å…¨å¸½": "æœªä¾è¦å®š æˆ´å®‰å…¨å¸½",
    }
    
    expanded = query
    for key, value in synonyms.items():
        if key in query:
            expanded += f" {value}"
    
    return expanded

# --- æ ¸å¿ƒï¼šé›™è»Œæœå°‹ ---
def hybrid_search(query: str):
    # 1. å…ˆåšåŒç¾©è©æ“´å±•
    expanded_query = expand_synonyms(query)
    print(f"ğŸ” æ“´å±•å¾Œæœå°‹è©: {expanded_query}")
    
    final_docs = []
    seen_ids = set()
    
    # è»Œé“ A: BM25 (æŠ“é—œéµå­—) - æé«˜åˆ° 30 ç­†
    if bm25:
        tokenized_query = list(jieba.cut(expanded_query))
        bm25_results = bm25.get_top_n(tokenized_query, all_laws, n=30)
        for doc in bm25_results:
            if doc['id'] not in seen_ids:
                final_docs.append({"text": doc['text'], "id": doc['id'], "score": 0.8})
                seen_ids.add(doc['id'])

    # è»Œé“ B: Vector (æŠ“èªæ„) - æé«˜åˆ° 30 ç­†
    vector_results = collection.query(
        query_texts=[expanded_query],
        n_results=30
    )
    
    if vector_results['documents'] and vector_results['documents'][0]:
        for i, doc_text in enumerate(vector_results['documents'][0]):
            doc_id = vector_results['ids'][0][i]
            if doc_id not in seen_ids:
                final_docs.append({"text": doc_text, "id": doc_id, "score": 0.7})
                seen_ids.add(doc_id)
            else:
                # é‡ç–ŠåŠ åˆ†
                for item in final_docs:
                    if item['id'] == doc_id:
                        item['score'] += 0.5

    # è»Œé“ C: é—œéµå­—æš´åŠ›åŠ æ¬Š
    # ç¢ºä¿åŒ…å«ã€Œä½¿ç”¨è€…åŸå§‹é—œéµå­—ã€çš„æ³•æ¢æ’åœ¨æœ€å‰é¢
    keywords = list(jieba.cut(query))
    for item in final_docs:
        for kw in keywords:
            if len(kw) > 1 and kw in item['text']:
                item['score'] += 0.3

    final_docs.sort(key=lambda x: x['score'], reverse=True)
    
    # å–å‰ 15 æ¢çµ¦ AI (Gemini Context Window å¤ å¤§ï¼Œå¤šçµ¦ä¸€é»æ²’é—œä¿‚)
    print(f"ğŸ† æœ€çµ‚å‰ 3 å: {[item['id'] for item in final_docs[:3]]}")
    return "\n\n".join([item['text'] for item in final_docs[:15]])

# --- ä¿®æ”¹ query_gemini_rag å‡½å¼ (å¹½é»˜èˆ‡æ¡ˆä¾‹ç‰ˆ) ---
def query_gemini_rag(user_question: str):
    print(f"ğŸ‘¤ ä½¿ç”¨è€…: {user_question}")

    # Step 1: AI ç¿»è­¯å®˜ (ä¿æŒä¸è®Š)
    rewrite_model = genai.GenerativeModel('gemini-2.0-flash')
    rewrite_prompt = f"""
    è«‹å°‡ä½¿ç”¨è€…å•é¡Œæ”¹å¯«ç‚ºç²¾æº–çš„æ³•å¾‹æœå°‹å­—ä¸²ã€‚
    1. ä¿®æ­£éŒ¯å­— (å¦‚: æ‹’çµ•ä¹ç­– -> æ‹’çµ•é…’æ¸¬)ã€‚
    2. è£œå……ä¸»è© (å¦‚: æœªç¦®è®“ -> æ±½è»Šæ©Ÿè»Šé§•é§›æœªç¦®è®“è¡Œäºº)ã€‚
    3. åªè¼¸å‡ºæ”¹å¯«å¾Œçš„å­—ä¸²ã€‚
    å•é¡Œ: {user_question}
    """
    rewritten_query = rewrite_model.generate_content(rewrite_prompt).text.strip()
    print(f"âœ¨ AI æ”¹å¯«: {rewritten_query}")

    # Step 2: æœå°‹ (ä¿æŒä¸è®Š)
    context_text = hybrid_search(rewritten_query)
    
    if not context_text:
        context_text = "ç›®å‰è³‡æ–™åº«ä¸­ç„¡ç›¸é—œæ³•æ¢ã€‚"

    # Step 3: ç”Ÿæˆå›ç­” (âœ¨ é‡é»ä¿®æ”¹ï¼šåŠ å…¥å¹½é»˜äººè¨­èˆ‡æ¡ˆä¾‹è¦æ±‚)
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½**å¹½é»˜é¢¨è¶£ã€èªªè©±ç›´ç™½ï¼Œä½†å°ˆæ¥­åº¦æ»¿åˆ†**çš„å°ç£æ³•å¾‹ AI é¡§å•ã€‚
    ä½ çš„ä»»å‹™æ˜¯å°‡è‰±æ¾€çš„æ³•å¾‹æ¢æ–‡ï¼Œç¿»è­¯æˆé€£å°å­¸ç”Ÿéƒ½è½å¾—æ‡‚çš„ç™½è©±æ–‡ï¼Œä¸¦ä¸”å–œæ­¡ç”¨**ç”Ÿæ´»åŒ–çš„æ¯”å–»**ä¾†è§£é‡‹ã€‚
    
    ã€ç›¸é—œæ³•è¦è³‡æ–™åº«ã€‘ï¼š
    {context_text}
    
    ã€ä½¿ç”¨è€…å•é¡Œã€‘ï¼š
    {user_question}
    
    ã€å›ç­”è¦å‰‡ (è«‹åš´æ ¼éµå®ˆ Markdown æ ¼å¼)ã€‘ï¼š
    1. **çµè«–å…ˆè¡Œ (é‡é»é«˜äº®)**ï¼šç¬¬ä¸€è¡Œç›´æ¥çµ¦å‡ºç­”æ¡ˆï¼ˆé‡‘é¡ã€åˆ‘æœŸï¼‰ï¼Œä¸¦ç”¨ **ç²—é«”** æ¨™ç¤ºé‡é»ã€‚
    2. **ç™½è©±æ–‡è§£é‡‹ (ç¥æ¯”å–»)**ï¼š
       - è«‹ç”¨ã€Œ**ç°¡å–®ä¾†èªª**ã€é–‹é ­ï¼Œè§£é‡‹é€™æ¢æ³•å¾‹åœ¨å¹¹å˜›ã€‚
       - **å¿…é ˆä½¿ç”¨ä¸€å€‹ã€Œç”Ÿæ´»æ¯”å–»ã€**ï¼ˆä¾‹å¦‚ï¼šæŠŠã€Œç°½å¥‘ç´„ã€æ¯”å–»æˆã€Œè¨‚å¤–é€ã€ï¼ŒæŠŠã€Œä¾µæ¬Šã€æ¯”å–»æˆã€Œå¼„å£åˆ¥äººçš„ç©å…·ã€ï¼‰ã€‚è®“ä½¿ç”¨è€…æœƒå¿ƒä¸€ç¬‘ã€‚
    3. **æƒ…å¢ƒæ¨¡æ“¬ (å°åŠ‡å ´)**ï¼š
       - è«‹å‰µé€ ä¸€å€‹ç°¡çŸ­çš„æ¡ˆä¾‹ï¼ˆä¾‹å¦‚ï¼šã€Œå‡è¨­å¼µä¸‰ä»Šå¤©...ã€ï¼‰ï¼Œèªªæ˜åœ¨é€™ç¨®æƒ…æ³ä¸‹æœƒç™¼ç”Ÿä»€éº¼äº‹ï¼Œè®“ä½¿ç”¨è€…æ›´æœ‰å¸¶å…¥æ„Ÿã€‚
    4. **å¼•ç”¨æ³•æ¢**ï¼šåˆ—å‡ºä¾æ“šçš„æ³•æ¢åç¨±èˆ‡æ¢è™Ÿï¼Œä¸¦ç”¨ **ç²—é«”**ã€‚
    5. **æ¢åˆ—å¼åˆ†æ**ï¼šè‹¥æœ‰ç½°å‰‡ç´°é …ï¼ˆå¦‚æ©Ÿè»Š/æ±½è»Šï¼‰ï¼Œè«‹æ¢åˆ—æ¸…æ¥šã€‚
    6. **å…è²¬è²æ˜**ï¼šæœ€å¾ŒåŠ ä¸Šå¼•ç”¨å€å¡Šã€‚
    
    èªæ°£è¦æ±‚ï¼šå°ˆæ¥­ä¸­å¸¶é»å¹½é»˜ï¼Œæº«æš–ä¸”å¥½æ‡‚ï¼Œä¸è¦åƒæ©Ÿå™¨äººã€‚
    """

    # generation_config è¨­å®š temperature=0.7 è®“å®ƒç¨å¾®æœ‰å‰µæ„ä¸€é» (é è¨­é€šå¸¸è¼ƒä½)
    answer_model = genai.GenerativeModel(
        'gemini-2.0-flash',
        generation_config=genai.GenerationConfig(temperature=0.7) 
    )
    response = answer_model.generate_content(system_prompt)
    return response.text

@app.get("/")
def read_root():
    return {"message": "Legal AI Backend is Running!"}

@app.post("/chat")
async def chat(request: ChatRequest):
    try:
        ai_reply = query_gemini_rag(request.message)
        return {"reply": ai_reply}
    except Exception as e:
        print(f"Error: {e}")
        return {"reply": f"ç™¼ç”ŸéŒ¯èª¤: {str(e)}"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)